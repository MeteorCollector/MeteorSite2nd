<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>VLA RL Aug '25 B | MeteorCollectorBlogs</title><meta name=keywords content="autonomous driving,VLA,diffusion,reinforcement learning"><meta name=description content="八月中下旬看的文章放到这里"><meta name=author content="MeteorCollector"><link rel=canonical href=https://meteorcollector.github.io/MeteorSite2nd/posts/vla-rl-0818/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/MeteorSite2nd/assets/css/stylesheet.0d7bdc28d739006af4c34db0d8752b71957f7a99895e0760dab3fd0110b9496c.css integrity="sha256-DXvcKNc5AGr0w02w2HUrcZV/epmJXgdg2rP9ARC5SWw=" rel="preload stylesheet" as=style><link rel=icon href=https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://meteorcollector.github.io/MeteorSite2nd/posts/vla-rl-0818/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://meteorcollector.github.io/MeteorSite2nd/posts/vla-rl-0818/"><meta property="og:site_name" content="MeteorCollectorBlogs"><meta property="og:title" content="VLA RL Aug '25 B"><meta property="og:description" content="八月中下旬看的文章放到这里"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-18T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-18T00:00:00+00:00"><meta property="article:tag" content="Autonomous Driving"><meta property="article:tag" content="VLA"><meta property="article:tag" content="Diffusion"><meta property="article:tag" content="Reinforcement Learning"><meta property="og:image" content="https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="VLA RL Aug '25 B"><meta name=twitter:description content="八月中下旬看的文章放到这里"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://meteorcollector.github.io/MeteorSite2nd/posts/"},{"@type":"ListItem","position":2,"name":"VLA RL Aug '25 B","item":"https://meteorcollector.github.io/MeteorSite2nd/posts/vla-rl-0818/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"VLA RL Aug '25 B","name":"VLA RL Aug \u002725 B","description":"八月中下旬看的文章放到这里","keywords":["autonomous driving","VLA","diffusion","reinforcement learning"],"articleBody":"看看最近的论文（2025年八月下半月） 一般来讲，越靠上的越新\nVLA Video Generators are Robot Policies 直接从视频转化成动作，比较简单的思路\n“只要让大视频扩散模型学会“想象”机器人完成任务的全过程，再用一个小解码器把想象转成动作，就能用极少的演示数据获得远超传统模仿学习的泛化能力。”\n🎯 研究动机 行为克隆（BC） 需要海量真人演示，且跨物体/场景/任务迁移差。 互联网级视频扩散模型（Sora 类）已学会物理与语义先验，却只用来“看”，没拿来“干”。 能否把“生成未来帧”直接当成策略？——即 Video Generator ≈ Robot Policy。 🧩 方法框架：Video Policy 组成 作用 技术细节 Video U-Net (μθ) 想象未来 8–32 帧 以 SVD 为骨干，输入首帧+任务文本 → 生成“机器人完成任务”的视频 Action U-Net (αθ) 把想象转成动作 轻量 1D-CNN，从 μθ 中间特征解码 6-DoF 轨迹 + 夹爪 两阶段训练 先训视频，后训动作 冻结 μθ，仅用 50–300 条演示即可训动作头；梯度不回传，避免稀释视频先验 🏗️ 核心洞见 先训视频再训动作 \u003e 端到端联合训练（RoboCasa +9 %）。 视频预测 horizon 越长 → 对分布漂移越鲁棒（图 3）。 无动作视频也能辅助泛化：仅用 12 任务动作数据，但视频见过全部 24 任务 → 在未见任务上仍 \u003e0.5 成功率（图 4）。 📊 实验结果 Benchmark 演示数 Video Policy 最强基线 提升 RoboCasa (34 任务) 50 66 % 平均成功率 DP-ResNet 41 % +25 % Libero-10 50 94 % 平均成功率 UVA 90 % +4 % 真实世界 5 任务 200/任务 80–100 % （位置/物体/背景漂移） — 显著 🔍 泛化维度验证 维度 举例 成功率 说明 位置漂移 抽屉/杯子随机摆放 80–100 % 视频先验鲁棒 未见物体 异形杯、彩色 M\u0026M 70–90 % 形状/颜色泛化 背景变化 黑/红/蓝桌面 80 %± 透明杯在低对比度场景略降 ⚖️ 局限与展望 计算大：25 帧 × 256² 需 9 s（A100），未来靠蒸馏/加速可实时。 单臂单任务：暂未做多臂、长时程、语言指令。 模型单一：仅基于 SVD；后续可试更大视频-语言-动作预训练模型。 把“想象机器人怎么做”的像素级扩散模型，当成策略本体；再配一个极轻量的动作解码器，就能用 50 条演示打败 3000 条演示的传统 BC，并轻松泛化到全新物体、场景和任务。\nIRL-VLA: Training an Vision-Language-Action Policy via Reward World Model arxiv github (5天前开源，16stars)\n训练世界模型进行 RL 训练\n这篇论文提出了一种全新的端到端自动驾驶框架——IRL-VLA（Inverse Reinforcement Learning for Vision-Language-Action），它通过逆强化学习构建奖励世界模型（Reward World Model, RWM），实现了不依赖高保真仿真器的闭环强化学习训练，显著提升了自动驾驶系统在复杂场景下的表现。\n🎯 研究背景与挑战 当前主流的端到端自动驾驶系统（如UniAD、VAD、DiffusionDrive等）大多基于模仿学习（Imitation Learning），存在两个关键缺陷：\n开环训练：模型只是“模仿”人类驾驶行为，无法主动探索更优策略，容易复制数据集中的次优行为。 闭环训练困难：传统闭环训练依赖高保真仿真器，存在Sim2Real域差和计算开销巨大的问题。 🧩 IRL-VLA 的核心思想 IRL-VLA 提出了一种三阶段训练范式，巧妙避开了上述问题：\n阶段 名称 关键内容 阶段1 模仿预训练 构建一个强大的Vision-Language-Action（VLA）模型，通过人类驾驶数据进行模仿学习，建立基础策略。 阶段2 逆环境学习 通过逆强化学习（IRL）训练一个轻量级奖励世界模型（RWM），用于预测任意轨迹的奖励（如安全性、舒适性、效率）。 阶段3 闭环强化学习 利用RWM作为奖励来源，采用PPO（Proximal Policy Optimization）算法对VLA策略进行微调，实现不依赖仿真器的闭环训练。 🧠 技术细节亮点 VLA模型架构 语义推理模块：基于SennaVLM，处理多视角图像和语言指令，理解场景语义。 3D推理模块：将图像特征投影到BEV（鸟瞰图）空间，提取地图和动态目标信息。 统一扩散规划器：采用扩散模型生成多模态轨迹，具备更强的泛化能力。 奖励世界模型（RWM） 输入：多视角图像 + 预测轨迹。 输出：预测8个驾驶指标（如无碰撞、车道保持、交通灯合规等），加权得到最终奖励。 优势： 无需仿真器：直接基于真实数据预测奖励，避免Sim2Real域差。 轻量级：相比传统仿真器，计算效率提升显著。 强化学习训练 算法：PPO（稳定、样本高效）。 奖励来源：RWM实时预测。 策略优化：结合模仿学习和强化学习损失，避免灾难性遗忘。 📊 实验结果 NAVSIM v2挑战赛：在CVPR 2025自动驾驶大挑战中获得第二名，EDPMS得分45.0。 Navhard基准测试：EPDMS得分74.9，超越DiffusionDrive（63.2）、WOTE（66.7）等方法。 消融实验： 加入语义推理模块，性能提升1.4 EPDMS。 加入扩散规划器，性能提升3.0 EPDMS。 模仿损失权重为0.5时，强化学习与模仿学习达到最佳平衡。 🚀 贡献与意义 首次实现了不依赖仿真器的端到端VLA闭环强化学习。 提出了通用可扩展的奖励世界模型（RWM），为自动驾驶强化学习提供了新范式。 在多个基准测试中达到SOTA性能，验证了方法的有效性和泛化能力。 RL EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving 这一篇 navsim 搞的很高，对抗网络还挺有意思的\nEvaDrive 把“轨迹生成 + 多目标评价”做成一个可迭代的对抗博弈：生成器（Actor）不断提出候选轨迹，评价器（Critic）用多维奖励向量打分，二者在多轮 Pareto 优化里互相拉扯，最终产出的轨迹既安全舒适又高效，还不用人工标注偏好。\n🎯 研究痛点 模仿学习 / 单目标 RL → 只能学平均行为，缺乏多样性和迭代试错。 现有生成-评价框架 → 生成和评价是“一次性”流水线，缺少闭环。 GRPO/DPO → 把多维指标硬压成一个标量奖励，带来 scalarization bias。 🧩 方案总览 模块 职责 关键技术 Actor（生成器） 输出多样化轨迹 ① 自回归意图建模（保留时间因果）② 单步扩散精炼（空间灵活） Critic（评价器） 给轨迹打分 输出 K 维奖励向量（安全、舒适、效率…） 多轮优化机制 让 Actor 与 Critic 来回迭代 每轮从 Pareto 前沿采样轨迹作为下一轮 anchor Adversarial Policy Optimization (APO) 让 Actor 与 Critic 对抗 类似 GAN：Actor 想拿高分，Critic 把专家轨迹打更高 🏗️ 训练流程（算法 1 简述） for 轮次 t = 0…K-1: 1. Actor 生成 64 条候选轨迹 At 2. Critic 输出 K 维奖励 r(·) 3. 用 Fast Non-Dominated Sort 建 Pareto 前沿 Pt 4. 从 Pt 均匀采样 M 条作为下一轮 anchor 5. 下一轮 Actor 以 anchor 为条件继续产出 At+1 最后一轮取 Pareto 最优轨迹作为最终输出。\n📊 实验战绩 场景 关键指标 EvaDrive 对比基线 提升 NAVSIM v1 PDMS 94.9 DiffusionDrive 88.1 +6.8 NAVSIM v2 EPDMS 86.3 Hydra-MDP++ 85.6 +0.7 Bench2Drive Driving Score 64.96 DriveTransformer 65.02 基本打平，但风格可控 风格开关：只改权重 w，即可在同模型上切换 保守型：安全权重高，PDMS=93.5 激进型：效率权重高，PDMS=94.9 消融（表 3）：去掉对抗/多轮/Pareto 任一环节都会掉点。 🔍 亮点小结 第一次在自动驾驶里把“多目标 + 多轮 + 对抗”完整跑通。 无需人工偏好对，奖励直接来自仿真规则，规避 GRPO/DPO 的标注噪声。 单步扩散 + Pareto 采样，兼顾实时性与多样性。 EvaDrive 让车像人一样“多想几步”，在 NAVSIM 上拿下新的 SOTA，同时可按权重旋钮切换驾驶风格。\nReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction arxiv 暂未开源\n算是一个有趣的思路吧。它先把场景做三维重建，这样agent无论在场景里如何移动，都可以先渲染个大概。对于动态物体例如其他交通参与者，就用 diffusion 渲染出来。\n这篇论文提出了 ReconDreamer-RL，一个利用扩散模型重建驾驶场景、增强强化学习训练效果的端到端自动驾驶训练框架。\n🎯 核心问题 现有自动驾驶强化学习训练面临两大瓶颈：\n仿真环境不真实（Sim2Real Gap）\n游戏引擎缺乏真实传感器数据； 重建方法（如3DGS）在新轨迹/视角下渲染质量差。 训练数据分布偏差\n模仿学习（IL）数据集中缺乏“cut-in”“急刹”等corner cases； 强化学习冷启动困难，探索空间受限。 🧩 ReconDreamer-RL 的三件套 模块 作用 技术亮点 ReconSimulator 构建高质量、可交互的仿真环境 3DGS + 视频扩散模型（DriveRestorer）提升新视角渲染质量；加入运动学模型保证轨迹物理合理性 DAA（动态对抗代理） 自动生成 corner cases 控制周围车辆行为（如cut-in、急刹），增强策略鲁棒性 CTG（Cousin轨迹生成器） 解决数据偏差 对专家轨迹进行扩展与插值，生成更多“非直线”行为，构建 Cousin-nuScenes 数据集 🏗️ 两阶段训练流程 阶段 内容 阶段1：模仿学习 使用 CTG+DAA 生成的数据训练初始策略（行为克隆） 阶段2：强化学习 在 ReconSimulator 中闭环训练，DAA 实时生成新corner cases，策略通过PPO优化 📊 实验结果 方法 Collision Ratio ↓ 说明 VAD（模仿学习） 0.386 缺乏corner cases，闭环表现差 RAD（RL+3DGS） 0.238 有RL但仍受限于渲染质量和数据分布 ReconDreamer-RL 0.077 相比模仿学习 ↓5×，相比RAD ↓3× 在 cut-in、急刹、对向车道入侵等corner cases中，ReconDreamer-RL 显著优于其他方法。\n贡献总结 首次将视频扩散先验引入驾驶场景重建+强化学习，显著缩小Sim2Real Gap。 提出DAA与CTG，解决corner case缺失与数据分布偏差问题。 在 nuScenes 和 Waymo 上均验证有效，碰撞率降低5倍，渲染速度125 FPS，支持高效RL训练。 ReconDreamer-RL 用扩散模型“重建+增强”真实驾驶场景，自动生成corner cases，让自动驾驶策略在仿真中也能“见过世面”，从而更安全、更鲁棒。\nDiffusion VFP: Variational Flow-Matching Policy for Multi-Modal Robot Manipulation 没太懂…主要是太基础了，很数学，先放这里，之后再看。\n这篇论文提出了 VFP（Variational Flow-Matching Policy），目标很明确：\n让基于流匹配（flow-matching）的策略也能像扩散模型一样，建模机器人在复杂任务中“多模态”的动作分布，同时保持流匹配原有的超快推理速度。\n🎯 研究背景 扩散策略（diffusion policy） 能很好地建模“一个状态 → 多种合理动作”的多模态分布，但采样慢（需要20步去噪）。 流匹配（flow matching） 只需一步ODE积分，推理速度是扩散的5倍，但天生会把多模态平均成单峰分布，导致“动作模糊”甚至失败。 🧩 核心贡献 模块 作用 技术亮点 变分潜变量 z（Variational Latent Prior） 为每个模式分配一个“开关” 让流解码器不再平均所有动作，而是根据 z 生成对应模式 Kantorovich-Optimal Transport（K-OT） 显式对齐“专家分布 vs 预测分布” 避免漏掉任何专家模式，全局分布级匹配 专家混合解码器（MoE-Flow） 每个专家只学一个模式 低计算量、易并行，推理时只激活一个专家 “先用潜变量 z 选模式，再用对应专家做一步流匹配，最后用 OT 保证所有模式都被覆盖。”\n📊 实验结果 场景 VFP vs 最强基线 说明 Franka Kitchen（任务多模态） +11.5% 成功率 避免“来回切换任务”的犹豫行为 D3IL Avoid（路径多模态） +61.7% 成功率 避免“直撞障碍物”的平均路径 Adroit \u0026 Meta-World（大规模） +4~15% 平均 在复杂手物操作任务上更鲁棒 推理速度：比扩散模型快 4.6×，比 FlowPolicy 仅慢 5.6%。 模型大小：与基线相当，甚至略小。 其他 AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation 卢策吾老师它们开发的具身用交互环境，码\nAgentWorld 是一个面向家庭场景的全链路仿真 + 数据采集平台：\n先用程序自动生成可交互、可渲染、可物理模拟的客厅/卧室/厨房，再用 VR/键盘双模式远程操作轮式或人形机器人收集大规模演示数据，最后给出1000+ 条轨迹的 AgentWorld Dataset，让模仿学习算法（BC、ACT、Diffusion Policy、π0）都能在 sim 训练后几秒内 zero-/few-shot 迁移到真机。\n🎯 研究痛点 现有仿真器要么只做场景生成，要么只做任务数据集，缺少端到端、移动+操作一体化的流水线。 家庭环境布局、材质、光照、物体摆放高度可变，需要程序 + 人工可编辑的灵活生成。 真机采集耗时、危险；需要高保真 VR 远程操作来快速攒大规模演示。 🧩 平台能力总览 模块 子功能 技术要点 程序场景生成 4 步流水线 ① 布局自动生成（墙/楼梯/多楼层）② 语义资产库 \u003e9000 件（家具、可交互物体）③ PBR 材质随机配置（木/金属/陶瓷…）④ Isaac Sim PhysX 5.0 自动物理绑定 移动遥操作 双模式 • 键盘：轮式/人形底盘（vx, vy, vθ）• VR：手部关键点 → 逆运动学 → 机械臂/五指手 数据集 1.15 k 轨迹 基础任务（抓放、开闭、推拉）多阶段任务（客厅整理、卧室铺床、厨房热饭）4 种机器人形态（G1、H1、Franka、X-Trainer） 验证实验 sim→real π0 在 sim 预训练 + 3 条真机微调 → 29.3% 成功率 📊 关键结果 任务类别 最佳算法 成功率 备注 基础操纵 ACT 66–84% 短序列，动作块机制有效 多阶段长任务 π0 20–30% 语言+视觉预训练带来长程结构理解 sim→real 迁移 π0 29.3% 仅用 3 条真机演示即可收敛到可执行策略 🔍 亮点与局限 亮点 具体表现 一站式 场景生成 + VR 采集 + 数据集 + 迁移验证全链路打通 高保真 Unreal 渲染 + PBR 材质 + Isaac Sim 物理，缩小视觉/动力学差距 可扩展 新资产仅需一次语义标注，后续完全自动 局限 未来方向 软体/布料尚未支持 引入可变形物体引擎 复杂长任务仍需真机微调 提升纯合成数据泛化能力 🧭 一句话收尾 AgentWorld 让研究者 “一键生成客厅，戴上 VR 手柄，十分钟就能攒出 100 条高质量演示”，为家庭级移动操作提供了开箱即用的仿真-真机闭环工具链。\nCBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving arxiv 暂未开源\n感知模块的 MoE，很神秘\nCBDES-MoE 把传统 BEV 感知里“一条固定 CNN/Transformer 走到黑”的思路，升级成**“多条异构主干并行，由轻量级路由器按需激活”** 的 Mixture-of-Experts 架构。\n🎯 痛点与动机 单一主干（ResNet/Swin/ConvNeXt/PVT）无法通吃所有天气、光照、视角变化。 现有动态卷积/可变形注意力只能在微观层面调参，宏观结构仍旧死板。 MoE 在 NLP/通用视觉很火，但在多模态 BEV 感知里还没系统落地。 🧩 方案拆解（3 个关键词：异构专家、轻路由、软融合） 模块 作用 设计细节 4 个异构专家 提供“风格”多样的视觉表征 Swin-T、ResNet-50、ConvNeXt、PVT 各成一路，结构差异带来互补偏置 Self-Attention Router (SAR) 0.1 ms 内决定“选哪位专家” 小卷积 + 单层 MHA + MLP，输出 4-way softmax Top-1 稀疏激活 推理时只跑 1 个专家 训练用软加权（可导），推理用 top-1（快），算力从 K×→1× 软融合 \u0026 负载均衡 避免路由塌缩到一两个专家 soft fusion 保证梯度稳定；额外 L_balance 让 4 专家使用率尽量平均 即插即用 不改 BEVFusion 其余流程 把原来的“相机 backbone”整块替换成 CBDES-MoE，后面 view-transform、LiDAR 融合、检测头全部复用 📊 nuScenes 结果（3D 检测） 模型 mAP↑ NDS↑ 备注 BEVFusion-Swin 64.0 65.6 最强单主干 BEVFusion-ResNet 63.3 65.2 — BEVFusion-ConvNeXt 61.6 65.2 — BEVFusion-PVT 62.4 65.7 — CBDES-MoE (top-1) 65.6 69.8 只跑 1 个专家，仍比所有单主干高 1.6 mAP / 4.1 NDS 训练阶段 4 专家全部更新；推理阶段每张图片只激活 1 个，显存 \u0026 延迟几乎与单主干持平。 消融显示：去掉负载均衡 → mAP 掉 2.2 点；把 4 专家换成同构 4×ResNet → 提升微弱，说明“异构”才是关键。 ","wordCount":"882","inLanguage":"en","image":"https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-08-18T00:00:00Z","dateModified":"2025-08-18T00:00:00Z","author":[{"@type":"Person","name":"MeteorCollector"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://meteorcollector.github.io/MeteorSite2nd/posts/vla-rl-0818/"},"publisher":{"@type":"Organization","name":"MeteorCollectorBlogs","logo":{"@type":"ImageObject","url":"https://meteorcollector.github.io/MeteorSite2nd/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://meteorcollector.github.io/MeteorSite2nd/ accesskey=h title="Home (Alt + H)"><img src=https://meteorcollector.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://meteorcollector.github.io/MeteorSite2nd/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://meteorcollector.github.io/MeteorSite2nd/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://MeteorCollector.github.io title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://meteorcollector.github.io/MeteorSite2nd/>Home</a>&nbsp;»&nbsp;<a href=https://meteorcollector.github.io/MeteorSite2nd/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">VLA RL Aug '25 B</h1><div class=post-description>八月中下旬看的文章放到这里</div><div class=post-meta><span title='2025-08-18 00:00:00 +0000 UTC'>August 18, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;882 words&nbsp;·&nbsp;MeteorCollector</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#vla>VLA</a><ul><li><a href=#video-generators-are-robot-policies>Video Generators are Robot Policies</a></li><li><a href=#irl-vla-training-an-vision-language-action-policy-via-reward-world-model>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model</a></li></ul></li><li><a href=#rl>RL</a><ul><li><a href=#evadrive-evolutionary-adversarial-policy-optimization-for-end-to-end-autonomous-driving>EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving</a></li><li><a href=#recondreamer-rl-enhancing-reinforcement-learning-via-diffusion-based-scene-reconstruction>ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction</a></li></ul></li><li><a href=#diffusion>Diffusion</a><ul><li><a href=#vfp-variational-flow-matching-policy-for-multi-modal-robot-manipulation>VFP: Variational Flow-Matching Policy for Multi-Modal Robot Manipulation</a></li></ul></li><li><a href=#其他>其他</a><ul><li><a href=#agentworld-an-interactive-simulation-platform-for-scene-construction-and-mobile-robotic-manipulation>AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</a></li><li><a href=#cbdes-moe-hierarchically-decoupled-mixture-of-experts-for-functional-modules-in-autonomous-driving>CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=看看最近的论文2025年八月下半月>看看最近的论文（2025年八月下半月）<a hidden class=anchor aria-hidden=true href=#看看最近的论文2025年八月下半月>#</a></h1><blockquote><p>一般来讲，越靠上的越新</p></blockquote><h2 id=vla>VLA<a hidden class=anchor aria-hidden=true href=#vla>#</a></h2><h3 id=video-generators-are-robot-policies>Video Generators are Robot Policies<a hidden class=anchor aria-hidden=true href=#video-generators-are-robot-policies>#</a></h3><blockquote><p>直接从视频转化成动作，比较简单的思路</p></blockquote><blockquote><p>“<strong>只要让大视频扩散模型学会“想象”机器人完成任务的全过程，再用一个小解码器把想象转成动作，就能用极少的演示数据获得远超传统模仿学习的泛化能力。</strong>”</p></blockquote><h4 id=-研究动机>🎯 研究动机<a hidden class=anchor aria-hidden=true href=#-研究动机>#</a></h4><ol><li><strong>行为克隆（BC）</strong> 需要海量真人演示，且跨物体/场景/任务迁移差。</li><li><strong>互联网级视频扩散模型</strong>（Sora 类）已学会物理与语义先验，却只用来“看”，没拿来“干”。</li><li>能否把“生成未来帧”直接当成策略？——即 <strong>Video Generator ≈ Robot Policy</strong>。</li></ol><h4 id=-方法框架video-policy>🧩 方法框架：Video Policy<a hidden class=anchor aria-hidden=true href=#-方法框架video-policy>#</a></h4><table><thead><tr><th>组成</th><th>作用</th><th>技术细节</th></tr></thead><tbody><tr><td><strong>Video U-Net (μθ)</strong></td><td>想象未来 8–32 帧</td><td>以 SVD 为骨干，输入首帧+任务文本 → 生成“机器人完成任务”的视频</td></tr><tr><td><strong>Action U-Net (αθ)</strong></td><td>把想象转成动作</td><td>轻量 1D-CNN，从 μθ 中间特征解码 6-DoF 轨迹 + 夹爪</td></tr><tr><td><strong>两阶段训练</strong></td><td>先训视频，后训动作</td><td>冻结 μθ，仅用 50–300 条演示即可训动作头；梯度不回传，避免稀释视频先验</td></tr></tbody></table><h4 id=-核心洞见>🏗️ 核心洞见<a hidden class=anchor aria-hidden=true href=#-核心洞见>#</a></h4><ul><li><strong>先训视频再训动作</strong> > 端到端联合训练（RoboCasa +9 %）。</li><li><strong>视频预测 horizon 越长</strong> → 对分布漂移越鲁棒（图 3）。</li><li><strong>无动作视频也能辅助泛化</strong>：仅用 12 任务动作数据，但视频见过全部 24 任务 → 在未见任务上仍 >0.5 成功率（图 4）。</li></ul><h4 id=-实验结果>📊 实验结果<a hidden class=anchor aria-hidden=true href=#-实验结果>#</a></h4><table><thead><tr><th>Benchmark</th><th>演示数</th><th>Video Policy</th><th>最强基线</th><th>提升</th></tr></thead><tbody><tr><td><strong>RoboCasa (34 任务)</strong></td><td>50</td><td><strong>66 %</strong> 平均成功率</td><td>DP-ResNet 41 %</td><td>+25 %</td></tr><tr><td><strong>Libero-10</strong></td><td>50</td><td><strong>94 %</strong> 平均成功率</td><td>UVA 90 %</td><td>+4 %</td></tr><tr><td><strong>真实世界 5 任务</strong></td><td>200/任务</td><td>80–100 % （位置/物体/背景漂移）</td><td>—</td><td>显著</td></tr></tbody></table><h4 id=-泛化维度验证>🔍 泛化维度验证<a hidden class=anchor aria-hidden=true href=#-泛化维度验证>#</a></h4><table><thead><tr><th>维度</th><th>举例</th><th>成功率</th><th>说明</th></tr></thead><tbody><tr><td><strong>位置漂移</strong></td><td>抽屉/杯子随机摆放</td><td>80–100 %</td><td>视频先验鲁棒</td></tr><tr><td><strong>未见物体</strong></td><td>异形杯、彩色 M&amp;M</td><td>70–90 %</td><td>形状/颜色泛化</td></tr><tr><td><strong>背景变化</strong></td><td>黑/红/蓝桌面</td><td>80 %±</td><td>透明杯在低对比度场景略降</td></tr></tbody></table><h4 id=-局限与展望>⚖️ 局限与展望<a hidden class=anchor aria-hidden=true href=#-局限与展望>#</a></h4><ul><li><strong>计算大</strong>：25 帧 × 256² 需 9 s（A100），未来靠蒸馏/加速可实时。</li><li><strong>单臂单任务</strong>：暂未做多臂、长时程、语言指令。</li><li><strong>模型单一</strong>：仅基于 SVD；后续可试更大视频-语言-动作预训练模型。</li></ul><blockquote><p><strong>把“想象机器人怎么做”的像素级扩散模型，当成策略本体；再配一个极轻量的动作解码器，就能用 50 条演示打败 3000 条演示的传统 BC，并轻松泛化到全新物体、场景和任务。</strong></p></blockquote><h3 id=irl-vla-training-an-vision-language-action-policy-via-reward-world-model>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model<a hidden class=anchor aria-hidden=true href=#irl-vla-training-an-vision-language-action-policy-via-reward-world-model>#</a></h3><p><a href=https://arxiv.org/abs/2508.06571>arxiv</a> <a href=https://github.com/IRL-VLA/IRL-VLA>github (5天前开源，16stars)</a></p><blockquote><p>训练世界模型进行 RL 训练</p></blockquote><p>这篇论文提出了一种全新的端到端自动驾驶框架——<strong>IRL-VLA</strong>（Inverse Reinforcement Learning for Vision-Language-Action），它通过<strong>逆强化学习构建奖励世界模型（Reward World Model, RWM）</strong>，实现了<strong>不依赖高保真仿真器的闭环强化学习训练</strong>，显著提升了自动驾驶系统在复杂场景下的表现。</p><h4 id=-研究背景与挑战>🎯 <strong>研究背景与挑战</strong><a hidden class=anchor aria-hidden=true href=#-研究背景与挑战>#</a></h4><p>当前主流的端到端自动驾驶系统（如UniAD、VAD、DiffusionDrive等）大多基于<strong>模仿学习（Imitation Learning）</strong>，存在两个关键缺陷：</p><ol><li><strong>开环训练</strong>：模型只是“模仿”人类驾驶行为，无法主动探索更优策略，容易复制数据集中的次优行为。</li><li><strong>闭环训练困难</strong>：传统闭环训练依赖高保真仿真器，存在<strong>Sim2Real域差</strong>和<strong>计算开销巨大</strong>的问题。</li></ol><h4 id=-irl-vla-的核心思想>🧩 <strong>IRL-VLA 的核心思想</strong><a hidden class=anchor aria-hidden=true href=#-irl-vla-的核心思想>#</a></h4><p>IRL-VLA 提出了一种<strong>三阶段训练范式</strong>，巧妙避开了上述问题：</p><table><thead><tr><th>阶段</th><th>名称</th><th>关键内容</th></tr></thead><tbody><tr><td><strong>阶段1</strong></td><td>模仿预训练</td><td>构建一个强大的Vision-Language-Action（VLA）模型，通过人类驾驶数据进行模仿学习，建立基础策略。</td></tr><tr><td><strong>阶段2</strong></td><td>逆环境学习</td><td>通过<strong>逆强化学习（IRL）<strong>训练一个</strong>轻量级奖励世界模型（RWM）</strong>，用于预测任意轨迹的奖励（如安全性、舒适性、效率）。</td></tr><tr><td><strong>阶段3</strong></td><td>闭环强化学习</td><td>利用RWM作为奖励来源，采用<strong>PPO（Proximal Policy Optimization）<strong>算法对VLA策略进行微调，实现</strong>不依赖仿真器的闭环训练</strong>。</td></tr></tbody></table><h4 id=-技术细节亮点>🧠 <strong>技术细节亮点</strong><a hidden class=anchor aria-hidden=true href=#-技术细节亮点>#</a></h4><ol><li><strong>VLA模型架构</strong></li></ol><ul><li><strong>语义推理模块</strong>：基于SennaVLM，处理多视角图像和语言指令，理解场景语义。</li><li><strong>3D推理模块</strong>：将图像特征投影到BEV（鸟瞰图）空间，提取地图和动态目标信息。</li><li><strong>统一扩散规划器</strong>：采用扩散模型生成多模态轨迹，具备更强的泛化能力。</li></ul><ol start=2><li><strong>奖励世界模型（RWM）</strong></li></ol><ul><li><strong>输入</strong>：多视角图像 + 预测轨迹。</li><li><strong>输出</strong>：预测8个驾驶指标（如无碰撞、车道保持、交通灯合规等），加权得到最终奖励。</li><li><strong>优势</strong>：<ul><li><strong>无需仿真器</strong>：直接基于真实数据预测奖励，避免Sim2Real域差。</li><li><strong>轻量级</strong>：相比传统仿真器，计算效率提升显著。</li></ul></li></ul><ol start=3><li><strong>强化学习训练</strong></li></ol><ul><li><strong>算法</strong>：PPO（稳定、样本高效）。</li><li><strong>奖励来源</strong>：RWM实时预测。</li><li><strong>策略优化</strong>：结合模仿学习和强化学习损失，避免灾难性遗忘。</li></ul><h4 id=-实验结果-1>📊 <strong>实验结果</strong><a hidden class=anchor aria-hidden=true href=#-实验结果-1>#</a></h4><ul><li><strong>NAVSIM v2挑战赛</strong>：在CVPR 2025自动驾驶大挑战中获得<strong>第二名</strong>，EDPMS得分45.0。</li><li><strong>Navhard基准测试</strong>：EPDMS得分74.9，超越DiffusionDrive（63.2）、WOTE（66.7）等方法。</li><li><strong>消融实验</strong>：<ul><li>加入语义推理模块，性能提升1.4 EPDMS。</li><li>加入扩散规划器，性能提升3.0 EPDMS。</li><li>模仿损失权重为0.5时，强化学习与模仿学习达到最佳平衡。</li></ul></li></ul><h4 id=-贡献与意义>🚀 <strong>贡献与意义</strong><a hidden class=anchor aria-hidden=true href=#-贡献与意义>#</a></h4><ul><li><strong>首次</strong>实现了<strong>不依赖仿真器的端到端VLA闭环强化学习</strong>。</li><li>提出了<strong>通用可扩展的奖励世界模型（RWM）</strong>，为自动驾驶强化学习提供了新范式。</li><li>在多个基准测试中达到<strong>SOTA性能</strong>，验证了方法的有效性和泛化能力。</li></ul><h2 id=rl>RL<a hidden class=anchor aria-hidden=true href=#rl>#</a></h2><h3 id=evadrive-evolutionary-adversarial-policy-optimization-for-end-to-end-autonomous-driving>EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving<a hidden class=anchor aria-hidden=true href=#evadrive-evolutionary-adversarial-policy-optimization-for-end-to-end-autonomous-driving>#</a></h3><blockquote><p>这一篇 navsim 搞的很高，对抗网络还挺有意思的</p></blockquote><p>EvaDrive 把“轨迹生成 + 多目标评价”做成一个<strong>可迭代的对抗博弈</strong>：生成器（Actor）不断提出候选轨迹，评价器（Critic）用<strong>多维奖励向量</strong>打分，二者在多轮 Pareto 优化里互相拉扯，最终产出的轨迹既安全舒适又高效，还不用人工标注偏好。</p><h4 id=-研究痛点>🎯 研究痛点<a hidden class=anchor aria-hidden=true href=#-研究痛点>#</a></h4><ol><li><strong>模仿学习 / 单目标 RL</strong> → 只能学平均行为，缺乏多样性和迭代试错。</li><li><strong>现有生成-评价框架</strong> → 生成和评价是“一次性”流水线，缺少闭环。</li><li><strong>GRPO/DPO</strong> → 把多维指标硬压成一个标量奖励，带来 scalarization bias。</li></ol><h4 id=-方案总览>🧩 方案总览<a hidden class=anchor aria-hidden=true href=#-方案总览>#</a></h4><table><thead><tr><th>模块</th><th>职责</th><th>关键技术</th></tr></thead><tbody><tr><td><strong>Actor（生成器）</strong></td><td>输出多样化轨迹</td><td>① 自回归意图建模（保留时间因果）② 单步扩散精炼（空间灵活）</td></tr><tr><td><strong>Critic（评价器）</strong></td><td>给轨迹打分</td><td>输出 <strong>K 维奖励向量</strong>（安全、舒适、效率…）</td></tr><tr><td><strong>多轮优化机制</strong></td><td>让 Actor 与 Critic 来回迭代</td><td>每轮从 Pareto 前沿采样轨迹作为下一轮 anchor</td></tr><tr><td><strong>Adversarial Policy Optimization (APO)</strong></td><td>让 Actor 与 Critic 对抗</td><td>类似 GAN：Actor 想拿高分，Critic 把专家轨迹打更高</td></tr></tbody></table><h4 id=-训练流程算法-1-简述>🏗️ 训练流程（算法 1 简述）<a hidden class=anchor aria-hidden=true href=#-训练流程算法-1-简述>#</a></h4><pre tabindex=0><code>for 轮次 t = 0…K-1:
    1. Actor 生成 64 条候选轨迹 At
    2. Critic 输出 K 维奖励 r(·)
    3. 用 Fast Non-Dominated Sort 建 Pareto 前沿 Pt
    4. 从 Pt 均匀采样 M 条作为下一轮 anchor
    5. 下一轮 Actor 以 anchor 为条件继续产出 At+1
</code></pre><p>最后一轮取 Pareto 最优轨迹作为最终输出。</p><h4 id=-实验战绩>📊 实验战绩<a hidden class=anchor aria-hidden=true href=#-实验战绩>#</a></h4><table><thead><tr><th>场景</th><th>关键指标</th><th>EvaDrive</th><th>对比基线</th><th>提升</th></tr></thead><tbody><tr><td>NAVSIM v1</td><td>PDMS</td><td><strong>94.9</strong></td><td>DiffusionDrive 88.1</td><td>+6.8</td></tr><tr><td>NAVSIM v2</td><td>EPDMS</td><td><strong>86.3</strong></td><td>Hydra-MDP++ 85.6</td><td>+0.7</td></tr><tr><td>Bench2Drive</td><td>Driving Score</td><td><strong>64.96</strong></td><td>DriveTransformer 65.02</td><td>基本打平，但风格可控</td></tr></tbody></table><ul><li><strong>风格开关</strong>：只改权重 w，即可在同模型上切换<ul><li>保守型：安全权重高，PDMS=93.5</li><li>激进型：效率权重高，PDMS=94.9</li></ul></li><li><strong>消融</strong>（表 3）：去掉对抗/多轮/Pareto 任一环节都会掉点。</li></ul><h4 id=-亮点小结>🔍 亮点小结<a hidden class=anchor aria-hidden=true href=#-亮点小结>#</a></h4><ol><li><strong>第一次</strong>在自动驾驶里把“多目标 + 多轮 + 对抗”完整跑通。</li><li><strong>无需人工偏好对</strong>，奖励直接来自仿真规则，规避 GRPO/DPO 的标注噪声。</li><li><strong>单步扩散 + Pareto 采样</strong>，兼顾实时性与多样性。</li></ol><blockquote><p>EvaDrive 让车像人一样“多想几步”，在 NAVSIM 上拿下新的 SOTA，同时可按权重旋钮切换驾驶风格。</p></blockquote><h3 id=recondreamer-rl-enhancing-reinforcement-learning-via-diffusion-based-scene-reconstruction>ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction<a hidden class=anchor aria-hidden=true href=#recondreamer-rl-enhancing-reinforcement-learning-via-diffusion-based-scene-reconstruction>#</a></h3><p><a href=https://arxiv.org/abs/2508.08170>arxiv</a> 暂未开源</p><blockquote><p>算是一个有趣的思路吧。它先把场景做三维重建，这样agent无论在场景里如何移动，都可以先渲染个大概。对于动态物体例如其他交通参与者，就用 diffusion 渲染出来。</p></blockquote><p>这篇论文提出了 <strong>ReconDreamer-RL</strong>，一个<strong>利用扩散模型重建驾驶场景、增强强化学习训练效果</strong>的端到端自动驾驶训练框架。</p><h4 id=-核心问题>🎯 <strong>核心问题</strong><a hidden class=anchor aria-hidden=true href=#-核心问题>#</a></h4><p>现有自动驾驶强化学习训练面临两大瓶颈：</p><ol><li><p><strong>仿真环境不真实</strong>（Sim2Real Gap）</p><ul><li>游戏引擎缺乏真实传感器数据；</li><li>重建方法（如3DGS）在新轨迹/视角下渲染质量差。</li></ul></li><li><p><strong>训练数据分布偏差</strong></p><ul><li>模仿学习（IL）数据集中缺乏“cut-in”“急刹”等corner cases；</li><li>强化学习冷启动困难，探索空间受限。</li></ul></li></ol><h4 id=-recondreamer-rl-的三件套>🧩 <strong>ReconDreamer-RL 的三件套</strong><a hidden class=anchor aria-hidden=true href=#-recondreamer-rl-的三件套>#</a></h4><table><thead><tr><th>模块</th><th>作用</th><th>技术亮点</th></tr></thead><tbody><tr><td><strong>ReconSimulator</strong></td><td>构建高质量、可交互的仿真环境</td><td>3DGS + 视频扩散模型（DriveRestorer）提升新视角渲染质量；加入运动学模型保证轨迹物理合理性</td></tr><tr><td><strong>DAA（动态对抗代理）</strong></td><td>自动生成 corner cases</td><td>控制周围车辆行为（如cut-in、急刹），增强策略鲁棒性</td></tr><tr><td><strong>CTG（Cousin轨迹生成器）</strong></td><td>解决数据偏差</td><td>对专家轨迹进行扩展与插值，生成更多“非直线”行为，构建 Cousin-nuScenes 数据集</td></tr></tbody></table><h4 id=-两阶段训练流程>🏗️ <strong>两阶段训练流程</strong><a hidden class=anchor aria-hidden=true href=#-两阶段训练流程>#</a></h4><table><thead><tr><th>阶段</th><th>内容</th></tr></thead><tbody><tr><td><strong>阶段1：模仿学习</strong></td><td>使用 CTG+DAA 生成的数据训练初始策略（行为克隆）</td></tr><tr><td><strong>阶段2：强化学习</strong></td><td>在 ReconSimulator 中闭环训练，DAA 实时生成新corner cases，策略通过PPO优化</td></tr></tbody></table><h4 id=-实验结果-2>📊 <strong>实验结果</strong><a hidden class=anchor aria-hidden=true href=#-实验结果-2>#</a></h4><table><thead><tr><th>方法</th><th>Collision Ratio ↓</th><th>说明</th></tr></thead><tbody><tr><td>VAD（模仿学习）</td><td>0.386</td><td>缺乏corner cases，闭环表现差</td></tr><tr><td>RAD（RL+3DGS）</td><td>0.238</td><td>有RL但仍受限于渲染质量和数据分布</td></tr><tr><td><strong>ReconDreamer-RL</strong></td><td><strong>0.077</strong></td><td>相比模仿学习 <strong>↓5×</strong>，相比RAD <strong>↓3×</strong></td></tr></tbody></table><blockquote><p>在 cut-in、急刹、对向车道入侵等corner cases中，ReconDreamer-RL 显著优于其他方法。</p></blockquote><h4 id=贡献总结><strong>贡献总结</strong><a hidden class=anchor aria-hidden=true href=#贡献总结>#</a></h4><ol><li><strong>首次</strong>将<strong>视频扩散先验</strong>引入<strong>驾驶场景重建+强化学习</strong>，显著缩小Sim2Real Gap。</li><li>提出<strong>DAA</strong>与<strong>CTG</strong>，解决corner case缺失与数据分布偏差问题。</li><li>在 nuScenes 和 Waymo 上均验证有效，<strong>碰撞率降低5倍</strong>，<strong>渲染速度125 FPS</strong>，支持高效RL训练。</li></ol><blockquote><p><strong>ReconDreamer-RL 用扩散模型“重建+增强”真实驾驶场景，自动生成corner cases，让自动驾驶策略在仿真中也能“见过世面”，从而更安全、更鲁棒。</strong></p></blockquote><h2 id=diffusion>Diffusion<a hidden class=anchor aria-hidden=true href=#diffusion>#</a></h2><h3 id=vfp-variational-flow-matching-policy-for-multi-modal-robot-manipulation>VFP: Variational Flow-Matching Policy for Multi-Modal Robot Manipulation<a hidden class=anchor aria-hidden=true href=#vfp-variational-flow-matching-policy-for-multi-modal-robot-manipulation>#</a></h3><blockquote><p>没太懂&mldr;主要是太基础了，很数学，先放这里，之后再看。</p></blockquote><p>这篇论文提出了 <strong>VFP</strong>（Variational Flow-Matching Policy），目标很明确：</p><blockquote><p>让<strong>基于流匹配（flow-matching）的策略</strong>也能像扩散模型一样，<strong>建模机器人在复杂任务中“多模态”的动作分布</strong>，同时保持流匹配原有的<strong>超快推理速度</strong>。</p></blockquote><h4 id=-研究背景>🎯 研究背景<a hidden class=anchor aria-hidden=true href=#-研究背景>#</a></h4><ul><li><strong>扩散策略（diffusion policy）</strong> 能很好地建模“一个状态 → 多种合理动作”的多模态分布，但<strong>采样慢</strong>（需要20步去噪）。</li><li><strong>流匹配（flow matching）</strong> 只需<strong>一步ODE积分</strong>，推理速度是扩散的5倍，但<strong>天生会把多模态平均成单峰分布</strong>，导致“动作模糊”甚至失败。</li></ul><h4 id=-核心贡献>🧩 核心贡献<a hidden class=anchor aria-hidden=true href=#-核心贡献>#</a></h4><table><thead><tr><th>模块</th><th>作用</th><th>技术亮点</th></tr></thead><tbody><tr><td><strong>变分潜变量 z（Variational Latent Prior）</strong></td><td>为每个模式分配一个“开关”</td><td>让流解码器<strong>不再平均所有动作</strong>，而是根据 z 生成对应模式</td></tr><tr><td><strong>Kantorovich-Optimal Transport（K-OT）</strong></td><td>显式对齐“专家分布 vs 预测分布”</td><td>避免漏掉任何专家模式，<strong>全局分布级匹配</strong></td></tr><tr><td><strong>专家混合解码器（MoE-Flow）</strong></td><td>每个专家只学一个模式</td><td>低计算量、易并行，<strong>推理时只激活一个专家</strong></td></tr></tbody></table><blockquote><p>“先用潜变量 z 选模式，再用对应专家做一步流匹配，最后用 OT 保证所有模式都被覆盖。”</p></blockquote><h4 id=-实验结果-3>📊 实验结果<a hidden class=anchor aria-hidden=true href=#-实验结果-3>#</a></h4><table><thead><tr><th>场景</th><th>VFP vs 最强基线</th><th>说明</th></tr></thead><tbody><tr><td><strong>Franka Kitchen</strong>（任务多模态）</td><td>+11.5% 成功率</td><td>避免“来回切换任务”的犹豫行为</td></tr><tr><td><strong>D3IL Avoid</strong>（路径多模态）</td><td>+61.7% 成功率</td><td>避免“直撞障碍物”的平均路径</td></tr><tr><td><strong>Adroit & Meta-World</strong>（大规模）</td><td>+4~15% 平均</td><td>在复杂手物操作任务上更鲁棒</td></tr></tbody></table><ul><li><strong>推理速度</strong>：比扩散模型快 <strong>4.6×</strong>，比 FlowPolicy 仅慢 <strong>5.6%</strong>。</li><li><strong>模型大小</strong>：与基线相当，甚至略小。</li></ul><h2 id=其他>其他<a hidden class=anchor aria-hidden=true href=#其他>#</a></h2><h3 id=agentworld-an-interactive-simulation-platform-for-scene-construction-and-mobile-robotic-manipulation>AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation<a hidden class=anchor aria-hidden=true href=#agentworld-an-interactive-simulation-platform-for-scene-construction-and-mobile-robotic-manipulation>#</a></h3><blockquote><p>卢策吾老师它们开发的具身用交互环境，码</p></blockquote><blockquote><p>AgentWorld 是一个<strong>面向家庭场景</strong>的<strong>全链路仿真 + 数据采集平台</strong>：<br>先用程序自动生成<strong>可交互、可渲染、可物理模拟</strong>的客厅/卧室/厨房，再用 VR/键盘双模式<strong>远程操作轮式或人形机器人</strong>收集大规模演示数据，最后给出<strong>1000+ 条轨迹的 AgentWorld Dataset</strong>，让模仿学习算法（BC、ACT、Diffusion Policy、π0）都能<strong>在 sim 训练后几秒内 zero-/few-shot 迁移到真机</strong>。</p></blockquote><h4 id=-研究痛点-1>🎯 研究痛点<a hidden class=anchor aria-hidden=true href=#-研究痛点-1>#</a></h4><ol><li><strong>现有仿真器</strong>要么只做<strong>场景生成</strong>，要么只做<strong>任务数据集</strong>，缺少<strong>端到端、移动+操作一体化</strong>的流水线。</li><li><strong>家庭环境</strong>布局、材质、光照、物体摆放高度可变，需要<strong>程序 + 人工可编辑</strong>的灵活生成。</li><li><strong>真机采集</strong>耗时、危险；需要<strong>高保真 VR 远程操作</strong>来快速攒大规模演示。</li></ol><h4 id=-平台能力总览>🧩 平台能力总览<a hidden class=anchor aria-hidden=true href=#-平台能力总览>#</a></h4><table><thead><tr><th>模块</th><th>子功能</th><th>技术要点</th></tr></thead><tbody><tr><td><strong>程序场景生成</strong></td><td>4 步流水线</td><td>① 布局自动生成（墙/楼梯/多楼层）② 语义资产库 >9000 件（家具、可交互物体）③ PBR 材质随机配置（木/金属/陶瓷…）④ Isaac Sim PhysX 5.0 自动物理绑定</td></tr><tr><td><strong>移动遥操作</strong></td><td>双模式</td><td>• 键盘：轮式/人形底盘（vx, vy, vθ）• VR：手部关键点 → 逆运动学 → 机械臂/五指手</td></tr><tr><td><strong>数据集</strong></td><td>1.15 k 轨迹</td><td>基础任务（抓放、开闭、推拉）多阶段任务（客厅整理、卧室铺床、厨房热饭）4 种机器人形态（G1、H1、Franka、X-Trainer）</td></tr><tr><td><strong>验证实验</strong></td><td>sim→real</td><td>π0 在 sim 预训练 + 3 条真机微调 → 29.3% 成功率</td></tr></tbody></table><h4 id=-关键结果>📊 关键结果<a hidden class=anchor aria-hidden=true href=#-关键结果>#</a></h4><table><thead><tr><th>任务类别</th><th>最佳算法</th><th>成功率</th><th>备注</th></tr></thead><tbody><tr><td><strong>基础操纵</strong></td><td>ACT</td><td>66–84%</td><td>短序列，动作块机制有效</td></tr><tr><td><strong>多阶段长任务</strong></td><td>π0</td><td>20–30%</td><td>语言+视觉预训练带来<strong>长程结构理解</strong></td></tr><tr><td><strong>sim→real 迁移</strong></td><td>π0</td><td>29.3%</td><td>仅用 3 条真机演示即可收敛到可执行策略</td></tr></tbody></table><h4 id=-亮点与局限>🔍 亮点与局限<a hidden class=anchor aria-hidden=true href=#-亮点与局限>#</a></h4><table><thead><tr><th>亮点</th><th>具体表现</th></tr></thead><tbody><tr><td><strong>一站式</strong></td><td>场景生成 + VR 采集 + 数据集 + 迁移验证全链路打通</td></tr><tr><td><strong>高保真</strong></td><td>Unreal 渲染 + PBR 材质 + Isaac Sim 物理，缩小视觉/动力学差距</td></tr><tr><td><strong>可扩展</strong></td><td>新资产仅需一次语义标注，后续完全自动</td></tr></tbody></table><table><thead><tr><th>局限</th><th>未来方向</th></tr></thead><tbody><tr><td>软体/布料尚未支持</td><td>引入可变形物体引擎</td></tr><tr><td>复杂长任务仍需真机微调</td><td>提升纯合成数据泛化能力</td></tr></tbody></table><h4 id=-一句话收尾>🧭 一句话收尾<a hidden class=anchor aria-hidden=true href=#-一句话收尾>#</a></h4><p>AgentWorld 让研究者 <strong>“一键生成客厅，戴上 VR 手柄，十分钟就能攒出 100 条高质量演示”</strong>，为家庭级移动操作提供了<strong>开箱即用的仿真-真机闭环工具链</strong>。</p><h3 id=cbdes-moe-hierarchically-decoupled-mixture-of-experts-for-functional-modules-in-autonomous-driving>CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving<a hidden class=anchor aria-hidden=true href=#cbdes-moe-hierarchically-decoupled-mixture-of-experts-for-functional-modules-in-autonomous-driving>#</a></h3><p><a href=https://arxiv.org/abs/2508.07838>arxiv</a> 暂未开源</p><blockquote><p>感知模块的 MoE，很神秘</p></blockquote><p>CBDES-MoE 把传统 BEV 感知里“一条固定 CNN/Transformer 走到黑”的思路，升级成**“多条异构主干并行，由轻量级路由器按需激活”** 的 Mixture-of-Experts 架构。</p><h4 id=-痛点与动机>🎯 痛点与动机<a hidden class=anchor aria-hidden=true href=#-痛点与动机>#</a></h4><ol><li>单一主干（ResNet/Swin/ConvNeXt/PVT）<strong>无法通吃</strong>所有天气、光照、视角变化。</li><li>现有动态卷积/可变形注意力只能在<strong>微观层面调参</strong>，宏观结构仍旧死板。</li><li>MoE 在 NLP/通用视觉很火，但在<strong>多模态 BEV 感知</strong>里还没系统落地。</li></ol><h4 id=-方案拆解3-个关键词异构专家轻路由软融合>🧩 方案拆解（3 个关键词：异构专家、轻路由、软融合）<a hidden class=anchor aria-hidden=true href=#-方案拆解3-个关键词异构专家轻路由软融合>#</a></h4><table><thead><tr><th>模块</th><th>作用</th><th>设计细节</th></tr></thead><tbody><tr><td><strong>4 个异构专家</strong></td><td>提供“风格”多样的视觉表征</td><td>Swin-T、ResNet-50、ConvNeXt、PVT 各成一路，结构差异带来互补偏置</td></tr><tr><td><strong>Self-Attention Router (SAR)</strong></td><td>0.1 ms 内决定“选哪位专家”</td><td>小卷积 + 单层 MHA + MLP，输出 4-way softmax</td></tr><tr><td><strong>Top-1 稀疏激活</strong></td><td>推理时只跑 1 个专家</td><td>训练用软加权（可导），推理用 top-1（快），算力从 K×→1×</td></tr><tr><td><strong>软融合 & 负载均衡</strong></td><td>避免路由塌缩到一两个专家</td><td>soft fusion 保证梯度稳定；额外 L_balance 让 4 专家使用率尽量平均</td></tr><tr><td><strong>即插即用</strong></td><td>不改 BEVFusion 其余流程</td><td>把原来的“相机 backbone”整块替换成 CBDES-MoE，后面 view-transform、LiDAR 融合、检测头全部复用</td></tr></tbody></table><h4 id=-nuscenes-结果3d-检测>📊 nuScenes 结果（3D 检测）<a hidden class=anchor aria-hidden=true href=#-nuscenes-结果3d-检测>#</a></h4><table><thead><tr><th>模型</th><th>mAP↑</th><th>NDS↑</th><th>备注</th></tr></thead><tbody><tr><td>BEVFusion-Swin</td><td>64.0</td><td>65.6</td><td>最强单主干</td></tr><tr><td>BEVFusion-ResNet</td><td>63.3</td><td>65.2</td><td>—</td></tr><tr><td>BEVFusion-ConvNeXt</td><td>61.6</td><td>65.2</td><td>—</td></tr><tr><td>BEVFusion-PVT</td><td>62.4</td><td>65.7</td><td>—</td></tr><tr><td><strong>CBDES-MoE (top-1)</strong></td><td><strong>65.6</strong></td><td><strong>69.8</strong></td><td>只跑 1 个专家，仍比所有单主干高 1.6 mAP / 4.1 NDS</td></tr></tbody></table><ul><li>训练阶段 4 专家全部更新；推理阶段每张图片只激活 1 个，<strong>显存 & 延迟几乎与单主干持平</strong>。</li><li>消融显示：去掉负载均衡 → mAP 掉 2.2 点；把 4 专家换成同构 4×ResNet → 提升微弱，说明“异构”才是关键。</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://meteorcollector.github.io/MeteorSite2nd/tags/autonomous-driving/>Autonomous Driving</a></li><li><a href=https://meteorcollector.github.io/MeteorSite2nd/tags/vla/>VLA</a></li><li><a href=https://meteorcollector.github.io/MeteorSite2nd/tags/diffusion/>Diffusion</a></li><li><a href=https://meteorcollector.github.io/MeteorSite2nd/tags/reinforcement-learning/>Reinforcement Learning</a></li></ul><nav class=paginav><a class=prev href=https://meteorcollector.github.io/MeteorSite2nd/posts/simlingo/><span class=title>« Prev</span><br><span>Simlingo 复现</span>
</a><a class=next href=https://meteorcollector.github.io/MeteorSite2nd/posts/vla-rl-0803/><span class=title>Next »</span><br><span>VLA RL Aug '25 A</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://meteorcollector.github.io/MeteorSite2nd/>MeteorCollectorBlogs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>