---
author: ["MeteorCollector"]
title: "Imitation Learning Revisited"
date: "2025-09-23"
description: "看一看模仿学习数据 scaling law 这边的论文"
summary: "看一看模仿学习数据 scaling law 这边的论文"
tags: ["autonomous driving"]
categories: ["autonomous driving"]
series: ["autonomous driving"]
ShowToc: true
TocOpen: true
---

导师教导我们要多做科学问题，避免做工程问题。这里来调研一个比较老的话题，关于自动驾驶方向在过去大行其道的训练方式——模仿学习能解决自动驾驶问题吗？或者，即使用了新范式，比如加上 VLM/VLA，在这基础上模仿学习，就能解决自动驾驶问题了吗？

为了解答这个问题，需要先看一些论文。

## Data Scaling Laws in Imitation Learning for Robotic Manipulation

Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, Yang Gao

[[arxiv]](https://arxiv.org/abs/2410.18647)

这是一篇 2024 年 10 月的文章。其实它就是一个探寻机器人方面 Scaling Law 的 paper，非常 straightforward：就是看看什么样的因素才能效率最高地提高机器人的泛化能力。

设置了三个变量：环境数量、物体数量和演示次数。具体来讲什么是环境数量和物体数量，其实也就是换 mesh 罢了。在这篇文章的 Appendix A 里有详细介绍。

概括论文这块我就不动笔了，让 Kimi 代劳了：

### 实验设置

作者围绕两个核心任务：**倒水（Pour Water）** 和 **鼠标摆放（Mouse Arrangement）**，设计了三大类实验，分别对应三种泛化维度：

| 实验类型 | 控制变量 | 目标 | 评估方式 |
|----------|----------|------|----------|
| **物体泛化实验** | 训练物体数量（N） | 策略能否泛化到**新物体** | 在**新物体**上测试策略表现 |
| **环境泛化实验** | 训练环境数量（M） | 策略能否泛化到**新环境** | 在**新环境**中测试策略表现 |
| **环境与物体联合泛化实验** | 训练环境-物体组合数量（M×N） | 策略能否泛化到**新环境+新物体** | 在**新环境+新物体**中测试策略表现 |


### 🔬 实验细节与变量控制

#### ✅ 数据收集方式
- 使用 **UMI 手持夹爪** 在真实环境中收集人类演示数据。
- 每个任务收集超过 **40,000 条演示数据**。
- 每个实验设置中，控制以下变量：
  - **环境数量（M）**：从 1 到 32 个不等。
  - **物体数量（N）**：从 1 到 32 个不等。
  - **演示次数（K）**：从 1 到 120 次不等。


### 📈 实验结果与结论

#### 1️⃣ 物体泛化实验

- **设计**：固定环境数量（1 个），逐步增加训练物体数量（1 → 32），每个物体演示 120 次。
- **结论**：
  - **增加训练物体数量显著提升策略对新物体的泛化能力**。
  - **当训练物体数量达到 32 个时，策略在新物体上的成功率超过 90%**。
  - **一旦物体数量足够，继续增加演示次数收益很小**。


#### 2️⃣ 环境泛化实验

- **设计**：固定物体数量（1 个），逐步增加训练环境数量（1 → 32），每个环境演示 120 次。
- **结论**：
  - **增加训练环境数量显著提升策略对新环境的泛化能力**。
  - **环境泛化比物体泛化更难**，需要更多环境才能达到相同性能。
  - **同样，演示次数超过一定阈值后，性能提升趋于饱和**。


#### 3️⃣ 环境与物体联合泛化实验

- **设计**：同时变化训练环境数量和物体数量（即环境-物体组合数量），每个组合演示 120 次。
- **结论**：
  - **联合泛化更具挑战性，但数据效率更高**。
  - **增加环境-物体组合数量能快速提升泛化能力**，且**演示次数的需求更低**（如 25% 演示即可达到与 100% 演示相近的性能）。

### 📊 幂律关系（Power Law）拟合

- 作者进一步将实验数据进行**对数坐标拟合**，发现：
  - **策略的泛化能力（以误差或成功率为指标）与训练环境数量、物体数量、环境-物体组合数量之间存在明显的幂律关系**。
  - **演示次数与泛化能力之间没有稳定的幂律关系**，性能会在一定演示次数后趋于饱和。

### 🧩 高效数据收集策略（核心结论）

基于上述实验结果，作者提出了一个**高效的数据收集策略**：

> **优先收集更多环境和物体，而不是在每个环境或物体上重复大量演示。**

- **推荐配置**：
  - **32 个环境 × 1 个物体/环境 × 50 次演示 = 1600 条数据**。
  - 即可训练出在新环境、新物体上成功率达 **90%** 的策略。

### ✅ 验证实验

- 作者将上述策略应用到两个新任务（**叠毛巾**、**拔充电器**）上：
  - **仅用 4 个数据收集者一个下午的时间**，收集到 32 个环境-物体组合，每个组合 50 次演示。
  - 训练出的策略在 **8 个新环境、2 个新物体/环境、5 次试验/物体** 的测试中，**成功率均接近 90%**。


### 🧠 总结一句话

> 作者通过系统地控制**环境数量、物体数量、演示次数**这三个变量，设计了一系列实验，最终得出**“环境和物体的多样性比演示次数更重要”**的核心结论，并基于这一结论提出了**高效、低成本的数据收集策略**，实现了机器人策略的**强泛化能力**。

没什么可细说的，实验和结论都非常直接。做出一个很好的科学问题，即使工作量不大，也可以中顶会啊，哈哈。虽然拉高演示次数可能确实在训练中益处不大，但是从另一个方面讲，作者未否定，假若见的数据种类够多，模仿学习就可以完全解决问题。这其实还是要打一个问号的。

## Preliminary Investigation into Data Scaling Laws for Imitation Learning-Based End-to-End Autonomous Driving

Yupeng Zheng, Zhongpu Xia, Qichao Zhang, Teng Zhang, Ben Lu, Xiaochuang Huo, Chao Han, Yixian Li, Mengjie Yu, Bu Jin, Pengxuan Yang, Yuhang Zheng, Haifeng Yuan, Ke Jiang, Peng Jia, Xianpeng Lang, Dongbin Zhao

[[arxiv]](https://arxiv.org/pdf/2412.02689)

24年12月的文章，看到了熟悉的名字。

偷个懒，AI 生成一下文章概述然后我写点自己的看法（

### 🎯 研究目标

本文旨在回答以下三个关键问题：

1. **端到端自动驾驶是否存在数据扩展规律？**
2. **数据量如何影响模型性能？**
3. **数据扩展是否能提升模型在新场景和新行为上的泛化能力？**


### 🧪 方法与实验设计

#### ✅ 数据集构建：ONE-Drive

- 收集了来自 **14 个城市**（如北京、上海、杭州等）的真实驾驶数据；
- 总共包含 **400 万条演示数据**，约 **3 万小时**；
- 每条数据包含：
  - 7 路摄像头图像（360° 环视）
  - 128 线激光雷达点云
  - 高精地图、交通灯、周围交通参与者信息等；
- 数据被划分为 **23 种场景类型**，如高速变道、红绿灯等待、匝道汇入等。

#### ✅ 模型架构：ONE-Model（基于 PARA-Drive）

> 他这个只构建了一个典型的范式哦，没有基于一些经典模型直接测。其实他这么搞，既弄数据集又弄模型，然后还搞一堆评测，工作量真的很大。看到论文这一长串作者也能意识到这一点。

- 输入：多视角图像 + 激光雷达
- 输出：未来 6 秒的规划轨迹
- 模块包括：
  - BEV 编码器（图像 + 激光雷达 → BEV 特征）
  - 并行解码器（地图、占用、检测、运动预测、规划）
  - 使用模仿学习（行为克隆）训练

> 经典 BEVFormer，一个好工作真的能吃好几年啊

#### ✅ 实验设置

- 构建了 **5 个不同规模的数据集**：1 万、5 万、70 万、200 万、400 万条演示；
- 使用 **开环评估（Open-loop）** 和 **闭环仿真（Closed-loop）** 两种方式评估；
- 开环指标：轨迹误差（ADE）
- 闭环指标：安全性、舒适性、规则遵守、效率、导航能力等五个维度

> 有趣的是，它这个闭环仿真并没有用 CARLA（当然也没有用 Bench2Drive），而是基于真实数据集用 3DGS (Gaussian Splating) 做的重建，在重建的世界里跑，属于是小 NAVSIM 了。我觉得确实不是非常的闭环。但是它后面揭示出闭环容易饱和，说明开闭环还是能分得开的。

### 🔍 关键发现

#### 1️⃣ 存在幂律扩展规律（Power Law）

- 在开环评估中，**轨迹误差与训练数据量之间呈现明显的幂律关系**；
- 拟合结果为：

  $$Y = 0.6833 \cdot X^{-0.188}, \quad r = -0.963$$

- 说明：**数据越多，轨迹预测越准确**，且符合幂律下降趋势。

> 暂且不表

#### 2️⃣ 开环 vs 闭环：扩展规律不一致

- 在闭环评估中，**数据量增加初期性能提升明显**，但在 **200 万条数据后趋于饱和**；
- 说明：**仅靠堆数据无法无限提升真实驾驶性能**，需要考虑其他因素（如数据分布、模型结构等）。

> 众所周知，开环数据是不靠谱的，所以看看闭环数据就能说明问题了。即使在这种伪 NAVSIM 的重建式闭环中表现也不好，那么真闭环呢？

#### 3️⃣ 数据分布比数据量更关键

- 作者发现某些“长尾场景”（如匝道变道、辅路汇入主路）在数据中占比少，但非常关键；
- 实验表明：
  - **将这些场景的数据量翻倍**，性能提升 **9.7%–16.9%**；
  - **将这些场景的数据量翻四倍**，性能提升 **22.8%–32.9%**；
- 结论：**有针对性地增加关键场景数据，比盲目增加整体数据更有效**。

> 这个和上一篇有类似的观点，但是这个提升是在 **开环场景** 中的提升，闭环虽然可能也有提升，但是幅度真的不好说。

#### 4️⃣ 数据扩展带来“组合泛化”能力

- 作者将某些场景（如高速绕行障碍、红绿灯等待）从训练中剔除，作为“新场景”；
- 结果发现：
  - **训练得少的模型（5 万条数据）无法泛化到新场景**；
  - **训练的多的模型（200 万条数据）即使没见过这些场景，也能通过组合已有知识进行合理规划**；
- 说明：**数据扩展能赋予模型“组合泛化”能力**，即**零样本泛化到新场景和新行为**。

> 也是开环指标，难说

### 🧠 总结一句话

> 这篇论文首次系统揭示了**端到端自动驾驶中的数据扩展规律**，证明：**数据量与性能之间存在幂律关系，但闭环性能会趋于饱和；数据分布的优化比盲目堆数据更关键；足够规模的数据能赋予模型组合泛化能力，实现新场景的零样本适应。**

这篇文章“闭环指标下饱和”这块很有意思，可惜又没在闭环上探索很多。

## Thoughts

确实有搞头